{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOE8aHfYvucw",
        "outputId": "7336849d-0c11-4f16-c6f8-f03391794aa2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_file):\n",
        "    data = pd.read_csv(data_file)\n",
        "\n",
        "    # Extract pixels and emotions\n",
        "    pixels = data['pixels'].tolist()\n",
        "    emotions = pd.get_dummies(data['emotion']).to_numpy()\n",
        "\n",
        "    # Convert pixels to 3D array and add channel dimension\n",
        "    faces = np.array([np.fromstring(pixel_sequence, dtype=int, sep=' ').reshape(48, 48) for pixel_sequence in pixels])\n",
        "    faces = np.expand_dims(faces, axis=-1)\n",
        "\n",
        "    return faces, emotions"
      ],
      "metadata": {
        "id": "rkAjF_hRAKcZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_file, reshape=True, dtype=torch.float32, seed=None):\n",
        "        self.data = pd.read_csv(data_file)\n",
        "        self.images = self.data['pixels'].apply(lambda x: np.fromstring(x, dtype=int, sep=' ').reshape(48, 48))\n",
        "        self.labels = self.data['emotion'].values.reshape(-1, 1)\n",
        "\n",
        "        if reshape:\n",
        "            self.images = np.stack(self.images).reshape(-1, 48*48)\n",
        "\n",
        "        self.labels = OneHotEncoder(sparse=False).fit_transform(self.labels)\n",
        "        self.images = torch.tensor(self.images, dtype=dtype) / 255.0\n",
        "        self.labels = torch.tensor(self.labels, dtype=dtype)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]\n"
      ],
      "metadata": {
        "id": "FmeitdgZANyY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.python.framework import dtypes, random_seed\n",
        "\n",
        "train_data = \"/content/drive/MyDrive/CV/fer2013.csv\""
      ],
      "metadata": {
        "id": "iiRAiYnn0i6G"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJuqEnP91PTg",
        "outputId": "2e0979a3-71c2-4d75-c74a-a4f2f42ad4cc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# Splitting into training and remaining sets\n",
        "train_data, remaining_data = train_test_split(dataset, test_size=(val_ratio + test_ratio))\n",
        "\n",
        "# Splitting remaining set into validation and test sets\n",
        "val_data, test_data = train_test_split(remaining_data, test_size=test_ratio / (val_ratio + test_ratio))\n",
        "\n",
        "# Create DataLoader objects for training, validation, and test sets\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "V2MAEaXz2IHm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # 1 input channel (grayscale), 32 output channels\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Downsampling layer\n",
        "\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # 32 input channels, 64 output channels\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Downsampling layer\n",
        "\n",
        "        # Third convolutional layer\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)  # 64 input channels, 128 output channels\n",
        "\n",
        "        # Fourth convolutional layer\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)  # 128 input channels, 128 output channels\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # Third downsampling layer\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size=4608, hidden_size=128, num_layers=1, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(128, 192)\n",
        "        self.fc2 = nn.Linear(192, 7)\n",
        "\n",
        "        # Classification layer and softmax layer for classification\n",
        "        self.classification_layer = nn.Linear(7, 7)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Activation function and dropout\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Adjust input tensor shape to match the convolutional layers' required dimensions\n",
        "        # Assume input image size is 48x48, with 1 channel (grayscale)\n",
        "        x = x.view(-1, 1, 48, 48)\n",
        "\n",
        "        # First convolutional layer\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        # Second convolutional layer\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Third convolutional layer\n",
        "        x = self.relu(self.conv3(x))\n",
        "\n",
        "        # Fourth convolutional layer\n",
        "        x = self.relu(self.conv4(x))\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        # Flatten the tensor for the fully connected layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # LSTM section\n",
        "        x, _ = self.lstm(x.unsqueeze(1))\n",
        "\n",
        "        # Choose the last output state of the LSTM\n",
        "        x = x[:, -1, :]\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "\n",
        "        x = self.classification_layer(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiate the CNN model\n",
        "model = CNN()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to the device\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move data to device\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, torch.argmax(labels, dim=1))  # Compute the loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update the weights\n",
        "\n",
        "    # Validate the model\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_losses = []\n",
        "    val_predictions = []\n",
        "    val_true_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Move data to device\n",
        "            outputs = model(images)  # Forward pass\n",
        "            val_loss = criterion(outputs, torch.argmax(labels, dim=1))  # Compute the loss\n",
        "            val_losses.append(val_loss.item())\n",
        "            val_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            val_true_labels.extend(torch.argmax(labels, dim=1).cpu().numpy())\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
        "\n",
        "    # Print epoch information\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "          f'Training Loss: {loss.item():.4f}, '\n",
        "          f'Validation Loss: {sum(val_losses)/len(val_losses)}, '\n",
        "          f'Validation Accuracy: {val_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlIIMpWL2dA2",
        "outputId": "12ccdc64-bbc3-4675-d45b-95c9971c7219"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Training Loss: 1.9069, Validation Loss: 1.897296810996603, Validation Accuracy: 0.2478\n",
            "Epoch [2/100], Training Loss: 1.8160, Validation Loss: 1.8884951101957694, Validation Accuracy: 0.2478\n",
            "Epoch [3/100], Training Loss: 1.8334, Validation Loss: 1.8292520758668347, Validation Accuracy: 0.3192\n",
            "Epoch [4/100], Training Loss: 1.8230, Validation Loss: 1.7972438391849135, Validation Accuracy: 0.3561\n",
            "Epoch [5/100], Training Loss: 1.8049, Validation Loss: 1.7797857840385662, Validation Accuracy: 0.3801\n",
            "Epoch [6/100], Training Loss: 1.7040, Validation Loss: 1.7583414316177368, Validation Accuracy: 0.3992\n",
            "Epoch [7/100], Training Loss: 1.7206, Validation Loss: 1.746565298921258, Validation Accuracy: 0.4117\n",
            "Epoch [8/100], Training Loss: 1.7970, Validation Loss: 1.737416736472993, Validation Accuracy: 0.4202\n",
            "Epoch [9/100], Training Loss: 1.6586, Validation Loss: 1.7343042947836882, Validation Accuracy: 0.4202\n",
            "Epoch [10/100], Training Loss: 1.7441, Validation Loss: 1.725534547010117, Validation Accuracy: 0.4332\n",
            "Epoch [11/100], Training Loss: 1.7247, Validation Loss: 1.7303782242995043, Validation Accuracy: 0.4258\n",
            "Epoch [12/100], Training Loss: 1.8050, Validation Loss: 1.7197091233800854, Validation Accuracy: 0.4369\n",
            "Epoch [13/100], Training Loss: 1.7584, Validation Loss: 1.722289129121769, Validation Accuracy: 0.4334\n",
            "Epoch [14/100], Training Loss: 1.6314, Validation Loss: 1.7183983008537067, Validation Accuracy: 0.4371\n",
            "Epoch [15/100], Training Loss: 1.5504, Validation Loss: 1.7129105230760293, Validation Accuracy: 0.4462\n",
            "Epoch [16/100], Training Loss: 1.7077, Validation Loss: 1.710149805221332, Validation Accuracy: 0.4464\n",
            "Epoch [17/100], Training Loss: 1.6788, Validation Loss: 1.7081358587953466, Validation Accuracy: 0.4510\n",
            "Epoch [18/100], Training Loss: 1.6128, Validation Loss: 1.7081124676755195, Validation Accuracy: 0.4497\n",
            "Epoch [19/100], Training Loss: 1.5672, Validation Loss: 1.706659473611053, Validation Accuracy: 0.4490\n",
            "Epoch [20/100], Training Loss: 1.7122, Validation Loss: 1.7050171385149984, Validation Accuracy: 0.4522\n",
            "Epoch [21/100], Training Loss: 1.5736, Validation Loss: 1.7022922561013487, Validation Accuracy: 0.4557\n",
            "Epoch [22/100], Training Loss: 1.7152, Validation Loss: 1.7028732666602502, Validation Accuracy: 0.4535\n",
            "Epoch [23/100], Training Loss: 1.5309, Validation Loss: 1.698278753715154, Validation Accuracy: 0.4592\n",
            "Epoch [24/100], Training Loss: 1.5837, Validation Loss: 1.7017705384090807, Validation Accuracy: 0.4570\n",
            "Epoch [25/100], Training Loss: 1.7597, Validation Loss: 1.6984834776827569, Validation Accuracy: 0.4572\n",
            "Epoch [26/100], Training Loss: 1.4751, Validation Loss: 1.6968715698761347, Validation Accuracy: 0.4620\n",
            "Epoch [27/100], Training Loss: 1.5067, Validation Loss: 1.694498079062919, Validation Accuracy: 0.4618\n",
            "Epoch [28/100], Training Loss: 1.7156, Validation Loss: 1.6960549827158098, Validation Accuracy: 0.4611\n",
            "Epoch [29/100], Training Loss: 1.5992, Validation Loss: 1.6939756842054559, Validation Accuracy: 0.4663\n",
            "Epoch [30/100], Training Loss: 1.7517, Validation Loss: 1.6925158274949654, Validation Accuracy: 0.4652\n",
            "Epoch [31/100], Training Loss: 1.5686, Validation Loss: 1.693778738467651, Validation Accuracy: 0.4642\n",
            "Epoch [32/100], Training Loss: 1.5285, Validation Loss: 1.6908897909187002, Validation Accuracy: 0.4674\n",
            "Epoch [33/100], Training Loss: 1.5932, Validation Loss: 1.6926673481450278, Validation Accuracy: 0.4707\n",
            "Epoch [34/100], Training Loss: 1.4755, Validation Loss: 1.6926012800995416, Validation Accuracy: 0.4652\n",
            "Epoch [35/100], Training Loss: 1.6623, Validation Loss: 1.6911789609130317, Validation Accuracy: 0.4676\n",
            "Epoch [36/100], Training Loss: 1.5773, Validation Loss: 1.6841219268606964, Validation Accuracy: 0.4732\n",
            "Epoch [37/100], Training Loss: 1.5145, Validation Loss: 1.688195219406715, Validation Accuracy: 0.4713\n",
            "Epoch [38/100], Training Loss: 1.4941, Validation Loss: 1.6818201160995212, Validation Accuracy: 0.4774\n",
            "Epoch [39/100], Training Loss: 1.6006, Validation Loss: 1.6833253725040593, Validation Accuracy: 0.4722\n",
            "Epoch [40/100], Training Loss: 1.4579, Validation Loss: 1.6863292928278093, Validation Accuracy: 0.4726\n",
            "Epoch [41/100], Training Loss: 1.5606, Validation Loss: 1.6880516688499225, Validation Accuracy: 0.4735\n",
            "Epoch [42/100], Training Loss: 1.5388, Validation Loss: 1.6788803952685474, Validation Accuracy: 0.4800\n",
            "Epoch [43/100], Training Loss: 1.5206, Validation Loss: 1.6774268453643166, Validation Accuracy: 0.4784\n",
            "Epoch [44/100], Training Loss: 1.3898, Validation Loss: 1.6716226734353241, Validation Accuracy: 0.4860\n",
            "Epoch [45/100], Training Loss: 1.3919, Validation Loss: 1.6704806258692544, Validation Accuracy: 0.4891\n",
            "Epoch [46/100], Training Loss: 1.5650, Validation Loss: 1.6700891834744336, Validation Accuracy: 0.4895\n",
            "Epoch [47/100], Training Loss: 1.4613, Validation Loss: 1.671126115251575, Validation Accuracy: 0.4884\n",
            "Epoch [48/100], Training Loss: 1.4767, Validation Loss: 1.6717112226599067, Validation Accuracy: 0.4856\n",
            "Epoch [49/100], Training Loss: 1.5381, Validation Loss: 1.6731485220102162, Validation Accuracy: 0.4850\n",
            "Epoch [50/100], Training Loss: 1.4550, Validation Loss: 1.6724933964260937, Validation Accuracy: 0.4889\n",
            "Epoch [51/100], Training Loss: 1.3651, Validation Loss: 1.6722249885987954, Validation Accuracy: 0.4839\n",
            "Epoch [52/100], Training Loss: 1.4528, Validation Loss: 1.6706740637502726, Validation Accuracy: 0.4839\n",
            "Epoch [53/100], Training Loss: 1.5005, Validation Loss: 1.6716806324275992, Validation Accuracy: 0.4828\n",
            "Epoch [54/100], Training Loss: 1.4460, Validation Loss: 1.6729721557459183, Validation Accuracy: 0.4823\n",
            "Epoch [55/100], Training Loss: 1.4081, Validation Loss: 1.6739206878391244, Validation Accuracy: 0.4811\n",
            "Epoch [56/100], Training Loss: 1.3646, Validation Loss: 1.6785350353760127, Validation Accuracy: 0.4739\n",
            "Epoch [57/100], Training Loss: 1.3092, Validation Loss: 1.6767095390861557, Validation Accuracy: 0.4810\n",
            "Epoch [58/100], Training Loss: 1.4274, Validation Loss: 1.6771440364905363, Validation Accuracy: 0.4797\n",
            "Epoch [59/100], Training Loss: 1.4405, Validation Loss: 1.6750631163106162, Validation Accuracy: 0.4804\n",
            "Epoch [60/100], Training Loss: 1.4370, Validation Loss: 1.6765338210664558, Validation Accuracy: 0.4763\n",
            "Epoch [61/100], Training Loss: 1.2932, Validation Loss: 1.6730440523497452, Validation Accuracy: 0.4826\n",
            "Epoch [62/100], Training Loss: 1.3664, Validation Loss: 1.670972582856579, Validation Accuracy: 0.4871\n",
            "Epoch [63/100], Training Loss: 1.3893, Validation Loss: 1.678426679069474, Validation Accuracy: 0.4758\n",
            "Epoch [64/100], Training Loss: 1.4031, Validation Loss: 1.6740484794921424, Validation Accuracy: 0.4815\n",
            "Epoch [65/100], Training Loss: 1.4655, Validation Loss: 1.6761871498717358, Validation Accuracy: 0.4813\n",
            "Epoch [66/100], Training Loss: 1.3568, Validation Loss: 1.6817418329814482, Validation Accuracy: 0.4759\n",
            "Epoch [67/100], Training Loss: 1.3320, Validation Loss: 1.673787301108682, Validation Accuracy: 0.4841\n",
            "Epoch [68/100], Training Loss: 1.3433, Validation Loss: 1.671379658597461, Validation Accuracy: 0.4854\n",
            "Epoch [69/100], Training Loss: 1.3838, Validation Loss: 1.6735098086870634, Validation Accuracy: 0.4811\n",
            "Epoch [70/100], Training Loss: 1.4043, Validation Loss: 1.679643314971021, Validation Accuracy: 0.4774\n",
            "Epoch [71/100], Training Loss: 1.3503, Validation Loss: 1.6727157556093657, Validation Accuracy: 0.4856\n",
            "Epoch [72/100], Training Loss: 1.3458, Validation Loss: 1.6764312834429318, Validation Accuracy: 0.4821\n",
            "Epoch [73/100], Training Loss: 1.4181, Validation Loss: 1.6787306888569036, Validation Accuracy: 0.4769\n",
            "Epoch [74/100], Training Loss: 1.3505, Validation Loss: 1.6739914713526618, Validation Accuracy: 0.4839\n",
            "Epoch [75/100], Training Loss: 1.3426, Validation Loss: 1.66658991398896, Validation Accuracy: 0.4908\n",
            "Epoch [76/100], Training Loss: 1.3596, Validation Loss: 1.6754071606686836, Validation Accuracy: 0.4836\n",
            "Epoch [77/100], Training Loss: 1.3489, Validation Loss: 1.6758217190849711, Validation Accuracy: 0.4813\n",
            "Epoch [78/100], Training Loss: 1.2399, Validation Loss: 1.670577174107704, Validation Accuracy: 0.4867\n",
            "Epoch [79/100], Training Loss: 1.2212, Validation Loss: 1.6710139181486954, Validation Accuracy: 0.4884\n",
            "Epoch [80/100], Training Loss: 1.2708, Validation Loss: 1.6744417537598921, Validation Accuracy: 0.4810\n",
            "Epoch [81/100], Training Loss: 1.2945, Validation Loss: 1.6767726783921733, Validation Accuracy: 0.4789\n",
            "Epoch [82/100], Training Loss: 1.2832, Validation Loss: 1.6692684289266373, Validation Accuracy: 0.4867\n",
            "Epoch [83/100], Training Loss: 1.3535, Validation Loss: 1.6716376190354838, Validation Accuracy: 0.4847\n",
            "Epoch [84/100], Training Loss: 1.3700, Validation Loss: 1.6683131583343596, Validation Accuracy: 0.4915\n",
            "Epoch [85/100], Training Loss: 1.4535, Validation Loss: 1.6696525240790914, Validation Accuracy: 0.4880\n",
            "Epoch [86/100], Training Loss: 1.2410, Validation Loss: 1.6614407237464859, Validation Accuracy: 0.4977\n",
            "Epoch [87/100], Training Loss: 1.3241, Validation Loss: 1.6709014699303892, Validation Accuracy: 0.4850\n",
            "Epoch [88/100], Training Loss: 1.2956, Validation Loss: 1.6698352063195945, Validation Accuracy: 0.4893\n",
            "Epoch [89/100], Training Loss: 1.2950, Validation Loss: 1.6721739698443892, Validation Accuracy: 0.4858\n",
            "Epoch [90/100], Training Loss: 1.2672, Validation Loss: 1.674855268918551, Validation Accuracy: 0.4839\n",
            "Epoch [91/100], Training Loss: 1.4149, Validation Loss: 1.6692941019520957, Validation Accuracy: 0.4893\n",
            "Epoch [92/100], Training Loss: 1.2657, Validation Loss: 1.6656054173700907, Validation Accuracy: 0.4936\n",
            "Epoch [93/100], Training Loss: 1.2887, Validation Loss: 1.668656344244466, Validation Accuracy: 0.4891\n",
            "Epoch [94/100], Training Loss: 1.3498, Validation Loss: 1.6675604673532338, Validation Accuracy: 0.4888\n",
            "Epoch [95/100], Training Loss: 1.3196, Validation Loss: 1.673035002319065, Validation Accuracy: 0.4862\n",
            "Epoch [96/100], Training Loss: 1.2645, Validation Loss: 1.6683814723110764, Validation Accuracy: 0.4904\n",
            "Epoch [97/100], Training Loss: 1.2505, Validation Loss: 1.6753145110677685, Validation Accuracy: 0.4821\n",
            "Epoch [98/100], Training Loss: 1.2874, Validation Loss: 1.6749855075362166, Validation Accuracy: 0.4821\n",
            "Epoch [99/100], Training Loss: 1.2930, Validation Loss: 1.6676018167529585, Validation Accuracy: 0.4906\n",
            "Epoch [100/100], Training Loss: 1.3269, Validation Loss: 1.6683623064199142, Validation Accuracy: 0.4910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Store test loss and predicted labels\n",
        "test_losses = []\n",
        "test_predictions = []\n",
        "test_true_labels = []\n",
        "\n",
        "# No need for gradient computation during testing\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Move data to the device (CPU or GPU)\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate test loss\n",
        "        test_loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
        "        test_losses.append(test_loss.item())\n",
        "\n",
        "        # Get the predicted labels from the model\n",
        "        test_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        # Get the true labels\n",
        "        test_true_labels.extend(torch.argmax(labels, dim=1).cpu().numpy())\n",
        "\n",
        "# Calculate average loss on the test set\n",
        "avg_test_loss = sum(test_losses) / len(test_losses)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
        "\n",
        "# Print test results\n",
        "print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnpsPDqmUgfh",
        "outputId": "43510d8a-0b65-42c6-b0dd-fe9c9320d53b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.6669, Test Accuracy: 0.4941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/drive/MyDrive/CV/cnn_lstm.pth'\n",
        "\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f'Model saved to {model_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHElUQ8hXkq6",
        "outputId": "e4dbd571-c41f-481f-c737-4fba147edaa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/CV/cnn_lstm.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def predict_image(model, image_path, transform):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path)\n",
        "    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Get the available device\n",
        "\n",
        "    # Move the input tensor to the appropriate device\n",
        "    image_tensor = image_tensor.to(device)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Move the model to the appropriate device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Perform the prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predicted_class = predicted.item()\n",
        "\n",
        "    # Display the image and predicted class\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Predicted class: {predicted_class}')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "aTn3iuyg7vWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMOTIONS = ['angry', 'disgusted', 'fearful', 'happy', 'sad', 'surprised', 'neutral']"
      ],
      "metadata": {
        "id": "UdxLc62h-NcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = \"/content/drive/MyDrive/COSC5470/angry2.jpeg\"\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((48, 48)),\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "predict_image(model, image, transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "zbR4z92k7ygF",
        "outputId": "5bbd379a-672d-4642-e18e-e791232309f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA4E0lEQVR4nO3debRd5Xke8OfsM093HnSvJJAQSEwCVwyxTbAsYrAlSL1qy+A4TTC2WxKMgS7jDF6NifG0EscOBIPreGWF1ZQkDRAnKcXYqIALmIAwM0gg0IDGO4/nnnHv3T8cfUUW7/MdEIUUnt9a/sO899tnnz2c956r9313Io7jGCIiIgCCt3oHRETkXw8lBRERcZQURETEUVIQERFHSUFERBwlBRERcZQURETEUVIQERFHSUFERBwlBXlTLVu2DJ/85Cfd/7/vvvuQSCRw3333vWX79It+cR//tWxL5M2gpPAOcvPNNyORSLj/5XI5rFy5EpdffjlGRkbe6t17Te6880784R/+4Vu9G287e/fuxYUXXoiuri50dHTgwx/+MLZv3/5W75a8iVJv9Q7Im+/aa6/F8uXLUavV8MADD+C73/0u7rzzTjzzzDMoFApv6r68733vQ7VaRSaTeU3r7rzzTtx4441KDG+g+fl5rFu3DjMzM/jiF7+IdDqNP/3TP8XatWvxxBNPoLe3963eRXkTKCm8A61fvx6nn346AOAzn/kMent78e1vfxv/+I//iF/7tV971TWVSgXFYvEN35cgCJDL5d7w7cprd9NNN2Hbtm145JFHcMYZZwD4+bVy8skn41vf+ha+/vWvv8V7KG8G/flIcM455wAAduzYAQD45Cc/iVKphJdeegkbNmxAuVzGr//6rwMAoijCddddh5NOOgm5XA6Dg4O49NJLMTU1dcg24zjGV7/6VSxZsgSFQgHr1q3Ds88+e9hrW/+m8PDDD2PDhg3o7u5GsVjEKaecguuvv97t34033ggAh/w57KA3eh8tURTh+uuvx+rVq5HL5dDf348PfehDePTRR801k5OTuPrqq7F69WqUSiV0dHRg/fr1ePLJJw/72RtuuAEnnXQSCoUCuru7cfrpp+Ov//qvXXxubg5XXXUVli1bhmw2i4GBAZx77rl47LHH3M8sLCxg69atGB8f976f2267DWeccYZLCABw/PHH41d+5Vfwd3/3d+0eFvn/nL4pCF566SUAOOTPA61WCx/84Afxy7/8y/iTP/kT92elSy+9FDfffDMuueQSXHHFFdixYwe+853v4PHHH8eDDz6IdDoNAPjSl76Er371q9iwYQM2bNiAxx57DOeddx4ajYZ3f+6++25ccMEFGBoawpVXXolFixZhy5YtuOOOO3DllVfi0ksvxb59+3D33Xfjr/7qrw5b/2bsIwB8+tOfxs0334z169fjM5/5DFqtFu6//3788z//s/sm9ou2b9+Of/iHf8DHPvYxLF++HCMjI/je976HtWvX4rnnnsPw8DAA4Pvf/z6uuOIKbNy4EVdeeSVqtRqeeuopPPzww/jEJz4BAPit3/ot3Hbbbbj88stx4oknYmJiAg888AC2bNmCNWvWAAAeeeQRrFu3Dtdccw39U1sURXjqqafwqU996rDYmWeeiR//+MeYm5tDuVxu69jI/8diecf4y7/8yxhAvGnTpnhsbCzevXt3/Ld/+7dxb29vnM/n4z179sRxHMcXX3xxDCD+vd/7vUPW33///TGA+JZbbjnkv991112H/PfR0dE4k8nE559/fhxFkfu5L37xizGA+OKLL3b/7d57740BxPfee28cx3HcarXi5cuXx0cffXQ8NTV1yOu8cluf/exn41e7fP9f7OOrueeee2IA8RVXXHFY7JXbO/roow/ZVq1Wi8MwPOTnd+zYEWez2fjaa691/+3DH/5wfNJJJ9F96OzsjD/72c/Snzl4fK+55hr6c2NjYzGAQ/bhoBtvvDEGEG/dupVuQ94e9Oejd6APfOAD6O/vx9KlS/Hxj38cpVIJP/jBD7B48eJDfu63f/u3D/n/t956Kzo7O3HuuedifHzc/e+0005DqVTCvffeCwDYtGkTGo0GPve5zx3yZ52rrrrKu2+PP/44duzYgauuugpdXV2HxF65LcubsY8AcPvttyORSOCaa645LMb2M5vNIgh+ftuFYYiJiQmUSiWsWrXqkD/7dHV1Yc+ePdi8ebO5ra6uLjz88MPYt2+f+TPvf//7Ecex9x/kq9Wq279fdPDffA7+jLy96c9H70A33ngjVq5ciVQqhcHBQaxatcp9UB2USqWwZMmSQ/7btm3bMDMzg4GBgVfd7ujoKABg165dAIDjjjvukHh/fz+6u7vpvh38U9bJJ5/c/ht6k/fx4H4ODw+jp6fnNe3fwX+HuOmmm7Bjxw6EYehir/zz3e/+7u9i06ZNOPPMM3HsscfivPPOwyc+8QmcddZZ7mf++I//GBdffDGWLl2K0047DRs2bMBv/uZv4phjjnlN+wQA+XweAFCv1w+L1Wq1Q35G3t6UFN6BzjzzTPNv3ge98jfag6IowsDAAG655ZZXXdPf3/+G7ePr9a99H7/+9a/jD/7gD/CpT30KX/nKV9DT04MgCHDVVVchiiL3cyeccAKef/553HHHHbjrrrtw++2346abbsKXvvQlfPnLXwYAXHjhhTj77LPxgx/8AD/+8Y/xzW9+E3/0R3+Ev//7v8f69etf03719PQgm81i//79h8UO/reD/94hb29KCtK2FStWYNOmTTjrrLPob41HH300gJ//1v7K31rHxsYOqwB6tdcAgGeeeQYf+MAHzJ+z/kTzZuzjwdf50Y9+hMnJydf0beG2227DunXr8Bd/8ReH/Pfp6Wn09fUd8t+KxSIuuugiXHTRRWg0GvjIRz6Cr33ta/j93/999yedoaEhXHbZZbjsssswOjqKNWvW4Gtf+9prTgpBEGD16tWvWjn18MMP45hjjtE/Mr9D6N8UpG0XXnghwjDEV77ylcNirVYL09PTAH7+bxbpdBo33HAD4jh2P3Pdddd5X2PNmjVYvnw5rrvuOre9g165rYM9E7/4M2/GPgLARz/6UcRx7H5rt/bzFyWTycPit956K/bu3XvIf5uYmDjk/2cyGZx44omI4xjNZhNhGGJmZuaQnxkYGMDw8PAhfwJ6LSWpGzduxObNmw9JDM8//zzuuecefOxjH/Oul7cHfVOQtq1duxaXXnopvvGNb+CJJ57Aeeedh3Q6jW3btuHWW2/F9ddfj40bN6K/vx9XX301vvGNb+CCCy7Ahg0b8Pjjj+OHP/zhYb8N/6IgCPDd734Xv/qrv4p3vetduOSSSzA0NIStW7fi2WefxY9+9CMAwGmnnQYAuOKKK/DBD34QyWQSH//4x9+UfQSAdevW4Td+4zfwZ3/2Z9i2bRs+9KEPIYoi3H///Vi3bh0uv/zyV113wQUX4Nprr8Ull1yC9773vXj66adxyy23HPbvAOeddx4WLVqEs846C4ODg9iyZQu+853v4Pzzz0e5XMb09DSWLFmCjRs34tRTT0WpVMKmTZuwefNmfOtb33LbabckFQAuu+wyfP/738f555+Pq6++Gul0Gt/+9rcxODiIz3/+895jIm8Tb13hk7zZDpakbt68mf7cxRdfHBeLRTP+53/+5/Fpp50W5/P5uFwux6tXr45/53d+J963b5/7mTAM4y9/+cvx0NBQnM/n4/e///3xM888c1iJ5i+WpB70wAMPxOeee25cLpfjYrEYn3LKKfENN9zg4q1WK/7c5z4X9/f3x4lE4rDy1DdyHy2tViv+5je/GR9//PFxJpOJ+/v74/Xr18c/+9nP3M+8Wknq5z//efeaZ511VvzQQw/Fa9eujdeuXet+7nvf+178vve9L+7t7Y2z2Wy8YsWK+Atf+EI8MzMTx3Ec1+v1+Atf+EJ86qmnumN06qmnxjfddNMh+9huSepBu3fvjjdu3Bh3dHTEpVIpvuCCC+Jt27a1tVbeHhJxTL7riojIO4r+TUFERBwlBRERcZQURETEUVIQERFHSUFERBwlBRERcdpuXlt3D29eaYZJOxbx3NNfqND4aKVE45Oz9iMkW+N8iFdheJ7GzznqBTPWkarRtUsykzR++/41NM4sK/Ft55P8mQCLs9NmrDO5QNc2Y37ZDKbtbS9Eh0/hfKWGZ9uLUva2fcZaHTQ+F/EnwHWR4zId8seY7qzxhrhKix+Xmaa9b7vnuujaiRn+xLwPr3zajD06cRRdu+v5RTT+3jXPm7H5Jn/P41W+3yOT/HziZfveT9X4xN3mcXwi7NkrXjRj9z27iu9Xk38eBqUmjReKhw8tPGh+nB+zXZ/+HRoH9E1BREReQUlBREQcJQUREXGUFERExFFSEBERR0lBREQcJQUREXHa7lNgfQgAkE6GZixI8Onc0zXeSzBf4/XMrTF7fbLXrukFgDVDu2k8iu282Zeeo2vvmTyexmdqdu35cGmWrh2r8d6N/hzvvwgSkRlLJ+xz2Y650D4foef3kEyiReORZ/1saB9T32uzPoQjVUry69Cn0sq87rVBwO+/f9q22oy9b9lLdO2BQd4r8M8/te+BY9fwe++U3n00/njMew3mcna9/8Ju/mjR/DP8M+mnqeVm7D3H82P20LPH0nhU5R/LxR77c6fZyT+n26FvCiIi4igpiIiIo6QgIiKOkoKIiDhKCiIi4igpiIiI03ZJKis59enI8hHT++d4WVtlhI+DRdHetxMWH6BL80k+prYrbZcp+sYhbxkbpPFMyi6/9JUgRp5yPF9J6t56txmbSfIx0EeiEPCR3r4y34kWL8WdISOsfaW2daRpnL5uix+z2RYfyz3d5CWQs2R0dtJT8l0u8HLYaTJ6/qG9y+jaC1Y8Q+N/X3+XGdvxIB/LveekLho/Y/hlGp8o2J8bL7T478PVgJ/P3FP2dfhUepiuXf9v7FHlAHDXT99F4yN77Xu3f3iarm2HvimIiIijpCAiIo6SgoiIOEoKIiLiKCmIiIijpCAiIo6SgoiIOG33Kfjq4mMSb3jGbk9M8Npz8DJsrF6xx4wdVZiia5tkNDYATDfteuX79x5D11YX+MjvTIfdp+AbVb6oyEdr55O8H6AR2ad+ksTakSL9AM0kf1/N2DOi3ddrENv7HngupClPr0E2YH0l/FyPN3ivzXyTr680X//o7JSnxyjLRkzP8v6K259aQ+MfP3WzGfub+pl0LZ7l/UsPNuzx1QBw1rIdZmxpL+9JGc/y/qW5epcZyz/USdfu7O6h8YHjxml86rF+M1brP7J7F9A3BREReQUlBRERcZQURETEUVIQERFHSUFERBwlBRERcZQURETEabuoNZu0a7QBoBnZ9eW7JnhdLuZ5zXB5Ma/Jz5FnIvjqw48tjtH4A6MrzNj8PK/hTqV5fXiL9CIU0rzPoJziM/LH67z3o5y2n3HRkeLPv6iGr79mvhrycz3nee4A64EAeC8B6zkB/M+wOBK+HolSmp/PVmT//lZt8mMakrUAkCDPY0hm+PEOPc8luHWL3cdw3upn6dr/lTuexoPt/N7+SeM4M3bmip10re+Yzg/b90g4za+znZuW0fjZH36cxu/u6jVjzb28t6Md+qYgIiKOkoKIiDhKCiIi4igpiIiIo6QgIiKOkoKIiDhKCiIi4rTdp1BI8br5XTPdZqw2zWvPcwMLNN5TqNL4VN2uC15SnKZrX6zYs8kBYN+EPRs9wR8xgUyG93awOfes7wMAxmq8D6EW8lO7QGryG1m+1tdr4Ht+BtPyPE+h5OnPKJLrdH+V13DXPccsmYjMWM7Tx9Od5de473yzOOszAIBiht+7vuctMK0W3+96xb7OnhxfTNeuHB6h8a2TS2k8u93+3Hk8u4SuPWPpyzRebdrXytRK/rt210/5szN+/LPVNN6/fNKMzW3mn2ft0DcFERFxlBRERMRRUhAREUdJQUREHCUFERFxlBRERMRpuyR1vMpLIKcm7XiQ4+V6i7r4aOxai+/mUWW7RKsvM0/X/tM2Xv7VnCHlYxm7RBEAagk+ijlRsEsJx+f5WOB6jh+TbIof85EF+3yxGABkPSWMrEQyFfBjFsW8ztdX7spKbStNfj4CX2knOaYdGV42HXre10KLl/myfSt5Sk7ZmHQA2F+xS3V956vpKSFOZe1jNlPJ07W+kd+lpfxzYx72+0q/wO+vLYUBGl/VY4/c3+LZ74UhXqLf+yg/poUV9qMCxgb4fd8OfVMQERFHSUFERBwlBRERcZQURETEUVIQERFHSUFERBwlBRERcdruUxib5bXrcWjnl6OGxvm25/i2BzvmaHxJbtqMbZ0bpGsbU7xmOBHa9eUxL+FGq8Jrz2usLt5TM59O8V4BXz3/XNXuv0gm+RsrlCo0nk/ZddQpMn4aACLwev65Bh877BtBzXR6eg2WFKbNWD5pv2cAmGzwuviapx/A12vARDH/3a+Qtve9Uue9HV0lfswYdg0CwPh+e2w9APQPT9N4tdcesx55eiTmnu2l8ZHT7fPhuzcrq3jvVLCDXyu7tts9FIuP4Z+17dA3BRERcZQURETEUVIQERFHSUFERBwlBRERcZQURETEUVIQERGn7T6FRo3/aP/gjBmre2rmG3W+7cXFaRqfaNp1vU/vWUzXIsXrw2NWpt3iNfVI8V6DmMzY9/UKlLN2DTbgr7kvddvrk54eiQp5ZgHAn2ng60MYW+A12mnvfH97+6PTvB8mP8h7DVgvwlyL97vkk/yZB4Gnf6MR2feIrzfjSJ7VkEsf2Xz+InnWg+95Cj7jL/Jegr5jJ8zYWB//zMnv4tf4zqeGzdipp79E17L7HgAqQ/w67X7CPt8dq15/P8tB+qYgIiKOkoKIiDhKCiIi4igpiIiIo6QgIiKOkoKIiDhtl6Sms7w0jZW1jU+V6drjhkZpvJTi5Xxbpu3x2PE+z2jsPl6GGKTtUsF0mR+TmFd2otW0D38mw7edDvh43s4ML00byNrjyOuk/BEAyim+7dG6fb6n67wMMZc6shLIiJQ/d5V5me7igl1WDQDV0C7t7EnzceLlJD9mMy1+XN5detGMTYcFuvbphaU0PpCZpXFmpsVfe3ulz4zN1fnobN/905zn5csTz9slq7ml/HzVe/g9kBu1f5/eNdNN1w6V+aMAnlvM15d32yWtLx7op2vboW8KIiLiKCmIiIijpCAiIo6SgoiIOEoKIiLiKCmIiIijpCAiIk7bfQrFPB/VPDZh16YXSnxth6emntWHA8C+yU4zFhb5SOJEwIuhIzIeO13kNfUdOf6+W5Gdk32jsTNJ3qeQCV5/vX/LM4q5GfPfJQLYx5T1swBAOcPfN9s2ALTIvvlee6LOa+6n6nbN/cpO3muzsmM/jb+nuI3GewO7xyKdGaNrzy3spPGhlD2quR7zPp4neQsRfpg41YwVPf1H29K85n7XLO9zyO22x183+j0ffX2e63C/3Vcy/wQf6d31K/xaKQzP03gr32HHRo9sHDmgbwoiIvIKSgoiIuIoKYiIiKOkICIijpKCiIg4SgoiIuIoKYiIiNN2n8LcPK9/jWO7nv+E/hG6tkFm4APAQsuuNwaAxpT9zIRMD++BaNb4IUiR50ikPb0CS8rTNJ5P2jXgITmevrUAkE7w/oy91S57redZDWM1u64d4P0A9ZAf79m653wE/H0lSbzpuc527+H15am8fS1cfvS9dO1Yiz9T5KHKcTQekPPZmeTPiTgpu4fGn2vax2VnYxFdO9K0e4QAYKxhv+98kvcpLDR5f5JPbZF9vjI7+XNWOv7NBI1PLLZ7JLqf4/fu4/sX03guw+/tere9/dwYf+126JuCiIg4SgoiIuIoKYiIiKOkICIijpKCiIg4SgoiIuK0XZLanOVlocessMtOJ2pFuvao0hSN3//SsTQe1OzclvCMSy518nK+iIy3ZjEAmG/y0b4pUmY4lJuha88q81HLAXjpZi22y/3+98zxdC0f1AzM1u1yP1+ZYbXB477zyUqja1V+DQdz/HYo9FbM2Pf3nE3XvrBnkMbTpPQZ4KXTec9o+kyKlxizcvNoih8zdPDyySBpn69wjp/r/qX8c8H7K23WvgeyU/xcVx62x6QDQPeZ42Ys8QwvbV4Y4Z+Hw6v4mPW95JTkxvn90Q59UxAREUdJQUREHCUFERFxlBRERMRRUhAREUdJQUREHCUFERFx2u5TyHTxWuhy2o5P1/nYbVavDwCZLXx99Rg+gpfxjb9e3G3XSg/leS/BMXm7lhkAlmdHzdhAco6uTSd4XfvW+jCNJ8kxP7nIRy2P1vno7OdeHjJj8YLnkgt4nXVQ4O87Du0+Bd9rZ2f52OH6k91mbHfNjgGAp5UACX4ZomuK1PtneT9MK8/fFzubhVF+byZC3msQZuzXjlJ8v+JHeK9AqYevry6y9913vGM+ZR110m8T9/H9KuzxXIcnenpWOuxrIW9/pLRN3xRERMRRUhAREUdJQUREHCUFERFxlBRERMRRUhAREUdJQUREnLb7FI7q47PND1TKZmx1L58P/pMdK2g8t0DDyJIi8FSK11mnkzx+VMF+36eXd9C1/alZGi8HNTO2u8lnsm+aOpHGnxhZTOOzs3bvx7KhCbp2ZNY+1z/fOKldJzPuAQC+5yVEvAYcZEZ/YS8vPi+M8NdO1ex9z8zy91Xt46/d6ODvq1mwY2HWc0w8UjX7fVd7+e+N2Rn+vlkvQs3TZ1Ab4Odj8X38WQ7zk/bHW+KjnqeC3N1Pw3O77XsgXsr7DLqf4tfC3plOGm/12e87+RzvG2mHvimIiIijpCAiIo6SgoiIOEoKIiLiKCmIiIijpCAiIo6SgoiIOG33KTRDXlvbDO380op57inez+fzNzpoGM2G/TY6ivN07UePepzG1xa3mrHhJB+S/3CNP9PgtvEzzNhTY/YzCQBgeqZI41HdU5P/YsaMHdiyhK4FLx8He/pFlR8SpCr8WglznhcnZfMpT79LYYwP2U9E9muPn+x5rgB/JAg8j8dAmLNjgWet730vdNv9As0y70OY8ZwP0oqDwPOMiWYXPx/738s/vrpesPdtZG8XX8tbIJCsk+d2kD4CAMjM82t8qmbfmz6ej9q26JuCiIg4SgoiIuIoKYiIiKOkICIijpKCiIg4SgoiIuK0XZI6MsPHJS/pmTZjD+5cztc+z2vTdm3g5X5Rwy6/XNnNR+Q+N89rJO86cBKNM7tG+PjraMouPUt6ytZyM3zscMIzoTpiZ94ziTk7zePF/XYpYf8TfMdqvfzF55by47JAxhbnpvi2iy/y8fA7P2KPU46yvDQzzNKw95hnyL7HvPoYtT7fvtnxOM3PV8IzyjzM2+uDJt/x9BSP1wd5LW7uITtWfp5/pswdw9932GFf47kM368oSeqLATRHef1yeak9kj9OdtG17dA3BRERcZQURETEUVIQERFHSUFERBwlBRERcZQURETEUVIQERGn7T6FMOT1yIP5OTM28shSujbKeObUelJXYsGuZ374gRP4a6d5DXdp+YwZO6F/hK7dl+mk8daCXa+cqnkK1z1hNmoZAAJyyFkMACrD/JhVltgnbOgBvrb7aft4A0B+nI9ZDx6268uzz+6kaxsn8JHhzbK972yUMsDHbgNAap6vj8md6huXXNzNt52bsvetMsw/IloF/tpR0t52q8SPSVTgo7ODAu8HqCyyb4Jar6d3g/RXAEAiY+9bKsX3u1ni5yNoeK6FmIw6L3o+GNqgbwoiIuIoKYiIiKOkICIijpKCiIg4SgoiIuIoKYiIiKOkICIiTtt9CsV8g8ZfnO4zY0vuGqdr60P8WQ3JKt/NxT+x64InV/GZ7Bd9+h4aPzZ3wIx984UP0rW1MT4XPUHm2IdFXicdVHk9cmaK53s2g7/WwWu4s57nEkT2YyIwcgY/lyt+xp9pkNu2i8aDQfuZB3G1xtfWeX15ftR+34VRfr6ipOd8VfhrV3vs85ngpwt9j9s9RAAQ5smzBWL+IIjKEL/O6vbHAuIM3/GgyPsQSiV+Puvd9v3X7OHbRoafzyR5zkQpx58PQz4qAQBB8/X3GvierdEOfVMQERFHSUFERBwlBRERcZQURETEUVIQERFHSUFERJy2S1JznvHWE9NkpPEaXpq5MMhzU2qBhhE0SfnYOl7iOB/ykrvv7DjHjE3PeuYGZz1lbbN2/Viqxo9JmPPUIXoEpMI4SSoUAaDpGXkctOySushThhh3d9B4/aTFNP7yJfYxj5q8FvC4/8KvcTaOvNrHz1eWjKcGgDDtGadMKijJJGUAQHURv05rPfZ1OHcU33htkJfSlpbOmrGuPC8pLWV4aWcq4PfX1r4uM5Ys83Md800jScZjDxbm6dqxsl02DQCZaX4tBaQG+cg+Ff5l+2/ANkRE5G1CSUFERBwlBRERcZQURETEUVIQERFHSUFERBwlBRERcdruU+jM8prikXG7BjzhqfmdP5VvO5niG6g/Z/caNJr8Ld7+wrtovFkn6yNew53fTmZIAyjtsauKs7O8/js3zkeZ+4RZ+/eBejc/ZguDfD5vi7SlNMu8krrZw2vqd2zkr339mX9jxt6dG6Nr3z3zeRrv/5m976Pv4ecr8PSdxGl+XHIH7PVpXhaPSc/5rPeSEe6eXht08nr/nkLVjHVl7RgANCJ+rg/M85H7MTnkxSL/zJmfI00pAKLQ3nh/jp+QsODpXyLnGgDCiIxR1+hsERF5IykpiIiIo6QgIiKOkoKIiDhKCiIi4igpiIiIo6QgIiJO230KvtnlmSk7v/T8dC9dO3v0EhpvdvIa7tSCPWy+Ns6f5VAYqPDXHrXXdz/Dc2pxhAzBBzBxgn3455bzguOhB/hr53bP0Hg8ZD+3gM3uB4DslKdvpNvet+wE3+9mmT/M4ZdWv0jjZ+fGSZT3lXzlQ7fS+B/tusiMpaf5+QqX8Lp430MRqnm7D4JX+/tlSnbPy9Ju+3kIADC1wO+vDtLfNF4t0rWn979M41t2vIvG0WEfs07Psxx8fQrNin2dBp7GrEwvf+3WKHk2DYD6rL1vxTfggQr6piAiIo6SgoiIOEoKIiLiKCmIiIijpCAiIo6SgoiIOEoKIiLitN2nkPEVr5M667Cbzz1veYprm6TeGABml9lvI7/PU/9dtJ/FAAC5A6T+POb7PbeEH96YbDrNy8NR7ef1/PXuXhqvkV6CgI/IR2UJP6b1XrtOu7yd/x7CziUAfLL3GRpPJ+zt12J+Ha1Ij9J49hy7B6LwNz107Ugvf18dA3wG/0DZjkeeHgc2fx8ABgpzZiwT8GNWSPPnelSa9jNFjuviz7e4d/dxNJ6c4sc0tdTuQWp6jkkqw993Y86+/0aqdg8QAAz38B6inYt4j0QwSe59fim0Rd8URETEUVIQERFHSUFERBwlBRERcZQURETEUVIQERGn/ZLUJC/RQsIuz6wu4SNywzwv7QwaPHdVhu1Y50t822GWl6RGdkUdZlbybUdZPkI3TtrrU/N8FHOryI9JlOb7RqsYPWVtYY6/r6hoXyupCt/41Gq+3z1JXroZkN9zOgNexuvz68sfMWN/9t4P0rWdz/DXnjmF3yOLO+0yxqXFKbo2n/TUGBOTjQKNd2X44O49811m7CdbV9K1cY3fA5klCzSeStnXaaVObmwAhRwvtW0k7c+NkQU++rqTjBMHgGSZn6/UqF2y2uSXUVv0TUFERBwlBRERcZQURETEUVIQERFHSUFERBwlBRERcZQURETEabtPIZXw1KaTst/KIl5vHJb5WO5Ela9vdtn7Vu/iawFeF18ftPctM+bbNhf22PX8+WF7nDEA1Ou87r3V5PvWUbbry2sNvu3GDO/tSI/b6zMVfryPO2U3jS9K8bHDEexrIZ3gtenlgNfcFwK7dv3cM56ia+8unUDjXY/wcck7dy4zY1sHl9K1saevJMHGRB/hKOaA9OJkCrwXoKOf1/PXm/zjqxWSMeo1fo2n07wvK5G34/M1fn+c1reHxl/YM0jj6Xn7pERtf6Lb9E1BREQcJQUREXGUFERExFFSEBERR0lBREQcJQUREXGUFERExHkDqlp/js3nr/bxYudEhtdRJ6c99cgle/3cCl5vXNjD6/kbw/a2mz38fQVVHi912/Pgi1lew91T4DX1pUydr89WzFgr4sfkoYnjaLxrix07cBbvU7ig70Uaj2L+e8xcZL/vAPyYTke8vnyqZQ+rr7T42lOW7aXxJ6tH03h+j30PpCr8mIQhvw6jFokH/HylPLP/O8r2NZ5O8vve9wyXZsvTi1Ow+xx8z1OozPO+kSR5VkNAni3z8zh/3+mX+bXEWqsSnsfetEPfFERExFFSEBERR0lBREQcJQUREXGUFERExFFSEBER5w0rSY3Sdp1Us5OvjZue3MQruJAg6zNDduklACxEdpkhAOR22aVr4QnzdG2GjSQGcEL/iBnrztilfACQDfi48dF6mca3z/SZsb27e+navkd4KWBl2C5xTPbyUtkdVXu/AOBA3nMxEb5SwFrMxykzi/PTND4a8PNRHODXaSW2r9Ogzu8fVi4OACDjreEpZ23V+EfIXGCXduZyvJw1V+DXeBB4zicZAe8r+T6SieF9JX4u65751s1u/r4CMhY/yaeNt0XfFERExFFSEBERR0lBREQcJQUREXGUFERExFFSEBERR0lBRESctvsUGp5xyjEJhzk+StbLUzR8JONifX0M9WTBft19dgwAKjleb/xEY7EZy2Z5jXZllo/2Dcb4aODMlP37QM8YP18Lg/yE1Abt95331KZ3pXl/xvb6AI1vwyCNM76x3KzPIRfw93ViaR+Ndx7NR6HfUV1txuJRPmo5LvAbhI2uD9KesfYpvu2AjN5uNPjHTyXFr+HA87nQiOzzOTreQdf2987ReL1p73s5zXtxfPKLeP9T4mW7V6dZPsLPWuibgoiIvIKSgoiIOEoKIiLiKCmIiIijpCAiIo6SgoiIOEoKIiLitN2nMLrA58EjZdfHRmxeO4BcFx8C3pos0Xicsbcfe4bJN6t8hn6ct+uwkz28l8BXk1+Zytv7Ne+p0Z7lpy5Z9czBL9nHrOppDIn4rqG4bMaMndx/gK69b99xNF7K8hrwIGG/r7k6r+fvyfMeiULKnsHvuz9apGYeAP7d0idp/N3Ld5ixn06tomsTVU+PETndETmeABC1+HWYIs8UiTzHZI6fDmQy/P6r1+19ixr8mCyQZzEAQJIcl8hz/wxn7fsDABKe/ot6j/3aS9bwfph26JuCiIg4SgoiIuIoKYiIiKOkICIijpKCiIg4SgoiIuIoKYiIiNN2n8JMlc/vj7NkrnqT555FXbM0/nK6yF+bzGxPeea9N8HrkROeHgu61lPjncrbddbhFK+pD1qvvw8BAIK6vT7M87WNHn5MV3ZPmbEnDwzTtQuj/FzPdPOeFvYcioV5fkxrZX47zI3Y/TJH3cHPR3aBH7O/XPsBGk+eZN8jmQFPQf9WT5/PnP2+m92e3xtLvBeH9QmFFX68w4D3EiTsxwoAAHKsT8jTv1SZ5593XZ32c1hYPwsANNnDZwD0l/nzFIq/NGnGzul7nq5th74piIiIo6QgIiKOkoKIiDhKCiIi4igpiIiIo6QgIiJO2yWpxSwvs5om6SXhKZ8cLMzR+M7iAI0nWvaLN+q85DRb9JSPNexD5Cs5nZ8s0Dgt1fVsu1XmJY5IRzQcp+2yuPR+XjKXrPLfJbbsXWTGOn5ijwsHgKMetMtZAWDk7G4anzrDLkNMsrJpAPMzfN86n7Gvpfx+XlbdKvF54x3baRiTBXs0d9jNR0ineHUlHbOemeDnutHi76tVJPsWeWZEe8rBa3O8xLietM9X0nN/wFOJPliyy0ZPKPHx8IUkH/9+0eJHabyctMuy9zW76Np26JuCiIg4SgoiIuIoKYiIiKOkICIijpKCiIg4SgoiIuIoKYiIiNN2n8KAp5dgb6vXDnpST1/GHkMLAMkSr8MO5+x65FaF9ylEeV4rHZCx3K2mp54/x+vi44yn14CIGp6xwgs8XtxlxyN+yOjYbQBIPm/3Z3S+xGu0fQYe5WOFE5E9JnrqRH65B/wyQ4OMah493e4jAIAwx49Zk0+3Rthp7xwbwQ4ArT6+7VbDvkHzu/kxS/P2DDTZ+GvP50Ls+YFEncfjsn1cWiFfm87zkeA9WXtceS7wrE3yz7v+FD+oz9eHzFhnskrXtkPfFERExFFSEBERR0lBREQcJQUREXGUFERExFFSEBERR0lBRESctvsU3te7jcZfGrCLoRde6KJrmzHPTUv6+Yz9XfP9NP7/jGfmehTy2vQE6YGIPWt9s+hT8/yYlvfY8+QnTvLU1Hfx/opWbK8/8Et8Bn7Q4nHPYyYQkMdjdLzI31dlMd94ddiue6+v4rXpuQJ/bofnbGNpya6Lb0b8XI9NdtB4SK61VpEfkxR5FgMAtJp2PCAxAAhL/JkHiR7e8xKTXp7A82yNbI6fz2Lq9ffbLET8Gk8m+PtOJ+x9j8i91y59UxAREUdJQUREHCUFERFxlBRERMRRUhAREUdJQUREnLZLUpemJ2n83cM7zdiPD6yma4/Jj9P4/iqZWQwg02WXhzXmMnTtkYhbnpzqqw5LktIzX2mZp2S19DJf3ija66MVfPxuX4ddHgkA+bRdzjc5ZI/VBoDQM9K4PsrXp6fs9ZlZT/lkJy9TTHbY76uvm4+WX9k9yrftqbVNkTLFesTHpLdCHp8J8mas6dmveJLPWc/M2OejVfBs2/fanvHxxW77Oo49pc2d+RqNR54yeqYZe0bue2rdy4G9b75tt0PfFERExFFSEBERR0lBREQcJQUREXGUFERExFFSEBERR0lBRESctvsUKhGv9z+tvMuMPbZkKV374OQKGi+k+Njh/s55M7ZvjI/VDgr2OGQAiEm/QJDhde3RPK/hjlmtsyddZ8Z5PXLA3xZmySHP5fnxXtrBR5nPNXNmLJ/hI4lnK/ZaAIhz/Jg3eu0a78aApzjdEw5n7PM5MttD187X+Ljkoc5ZGu/P2df40gI/H5OlIo0nA7sHYiZl9zAAQEB6AQCgeqBE41TOMzqb9fkASJA+h0KWX4d9+QqNM7WI3/dh4sh+Fy8n7WM+2TqC4/0v9E1BREQcJQUREXGUFERExFFSEBERR0lBREQcJQUREXGUFERExGm7T6EjyeeLN1v2pn5p0O5hAIDnZwdo/N3dO2i8QebFjw/wGu36DK+LDyr2tqOSpxkgyQvfE1V7274+g1SFPxugMszXY7ldhx145tg3In7ZrOqwnx3waJ33rDTmeT9MYoH3Z8R5u4+BHW8ACOr8mIYluy4+18vr9Us5+5kfADAyx+vLd0902Wt7ynStT0R6caKIH5NCjtf7L2TsY+Y7l+Ue3ivQX+LxSsO+llgPAwAE5PkVAJAO7OtswdPTlU7wXptazPscWB/EQsT7YdqhbwoiIuIoKYiIiKOkICIijpKCiIg4SgoiIuIoKYiIiNN2SWo6wWsk5yK7tPPk4h66tjdjjwUGgHrMd3P3bLcZSz7Oy/WGtvPSszBrl+RV+3npWWUp33actsvikrNHlq8bnfy1i2Q89lAHH+PMShgB4NExu+x0dLSTrk2PeMYO5z1lvnP2tRLl+TGBpyQ1UbfPSaPGr9GxGr8Ooypfn2jar719lpchJlL8mJW6FsxYs8rPx+T0EZRAekq2w5DfA7M1Xk5ea9rHNJPin2cLLX5vp0hZ6VyL71cxycuTp8MCjbOS1GbMy3zboW8KIiLiKCmIiIijpCAiIo6SgoiIOEoKIiLiKCmIiIijpCAiIk7bfQrb64M03pO0ew1841yH0tM0/u0nP0Djx37FHlsc73yCrk0s56Oco4x9iMqd/H3NjfH4+BoS5CXz3tHaPrWaXeu8o95L17b28jrq9Ly98+VJ/sZ878szdZjGmyV+ufteOz9u9zlkp/i28/v5mOdEg8ejon0tNTp5TX2jk+/bzAq7z6fkOd6x59fK+RPsfphE0Y4BQMJzD8xX+f3Fxn6nAt6z0ow8Y9bJ6O0g5v0X2SO8eelre0Z+t7X9I96CiIi8bSgpiIiIo6QgIiKOkoKIiDhKCiIi4igpiIiIo6QgIiJO230Kg55egrkwb8b6U3N0re9ZDb3/w942ACC258EH3V10aWvLizSeWjxkxpIz/PBl9vBC64VBe9vVQV5v3Gp5Zv/zUmm0xu1jWtjNa7SXPMrnwacqTTMW5vgxy740SuNIeebFR/ZxixdqfG2T183HDft9RTV+THzV44l027fiYdJ1/tqZNO9jKJ90rBlrlfjaZid/3sLCkP2+4j5+VNJJ3iSRz9jnA+DPUzhS1dB+3/kk369ykl+HgedqyZDPyyQ8N34b9E1BREQcJQUREXGUFERExFFSEBERR0lBREQcJQUREXGUFERExGm7kLcW8XrkXGDX5v5kdhVde8dzq2l8+QFe98tmzSeSnrno0zM0Hh6w6+aDzjJdC1LXDgC9z/aZsT2DfL9rA7y3Iz3N1xf22b8P9Gzh+53bPk7j4e69Zsx3wYUJ/ntKUCryDaSOoDY9y+fzJ4btZ4oEAd/vsMy37asuT5AZ/WGev+eFQd5rMLfE3vfcJN+zVJXHoy77Ou3rsZ/BAgDVBv/Mma/kaDyXt/tOwpj3+USe+FitZMaOKk7RtaHnIRTNmJ/PJHlmQjP29PG0Qd8URETEUVIQERFHSUFERBwlBRERcZQURETEUVIQERGn7fq9/c1uGv/eT84xY8f+NR8Ve/w0L01LTHrKRqem7bWeMsPIUzYah/b43miGjwSPPaOYmaiP7xeqvPQsyvBSwSypmsuNVunaxpIeGk922eV6yXF+Ln3iEh+jHuXs8svQNwa6xG+Hepd9zMMsL2H0CfmuodZrb79Z5ue62c/Ll5PklDQ7+PsqesbD57fbb6zew69hUoULAAgbfH01tu/9IODjqcM8/8zKkLHepSQfZV7wxH3YaO1C8Po/c/7v9kVERP6FkoKIiDhKCiIi4igpiIiIo6QgIiKOkoKIiDhKCiIi4rTdp8D6EABg6Y/souID7+Hjjpf92xEa33/zMTTe+1ebzVjc4jXaRyTmtc6JNC8+nz3aPvxxza6DBoCg6hkx3fLUzZMa8OqiAl06P8zrw9MLdn14qmr3MAAAmQoMAAhanpr8on1cWp5eghZ/22iWSK8Af1uIsny/Q09fSZS3r4d0F697T9T5bR4vsu8RT7cM5lJ8fHWSlM03Gny/Tlx0gMafri+m8daMff/FJc84fs/o7BS5UH1rh9N8tLaP7zEGR0rfFERExFFSEBERR0lBREQcJQUREXGUFERExFFSEBERR0lBRESctvsUup/m+aNGHrfwro3P0LX/efiHNH7tfzifxg/sOMWMJf/3k3QtIt4PkEiRQ5Tgx6R59moan1pNXjvgdesJ36x5T118IrJrqau9vA9hYYjXYafn7bjvuQGBp63E1w/AxsnHnqvdd8xaHfb5ijOeBgtyvAEgkedvPEGWx566+ESS71tM9i2b99Tzr6jQeP0Aaf5Y4BfDvvlOGk+m+L3LWnV8x8ynEdn3yEBmlq7tCPizGiZCfpHPhHbfVzbwdZb46ZuCiIg4SgoiIuIoKYiIiKOkICIijpKCiIg4SgoiIuK0XZIabeDjXos32mVUj+1fStfeVlxD4/9+4CEav/dPp83Ynbe8l67t3MnL2pjx1bx0s3VslW+gapfFBbP81ISdntrNFs/3jQ57+75yV3iq+Wq99gbCvGdEdNYzjrzE33c8T46bb9s1fj7jtL0+OcvXht2e8+U5pqUO+1o6sZ+Pnt822Ufj09N2iWPouY5anrHcyX67/DJs8GOWS73+Ml0AADndcZO/r0rdUy4Lu1y2v3+ObzuyR8sDQHgE5bJJ9qbbpG8KIiLiKCmIiIijpCAiIo6SgoiIOEoKIiLiKCmIiIijpCAiIk7bfQrXr/5bGr/kNy4xY4Wf8hG488t53e4D8ytpfH3HU2as8zd5r8DWyiCNb5+xa7yXZep07XQtT+MjC/a88ZiXcCPI8f6KaJ7n+3ov2TZ/W4g9Y73jZfYxTwa8jrq/Y4HGx6fKNJ7ut1+7s8ivhYlpPrI4QfY9zPMTlgj5+Yg9PRK5XrtmP5/k45KP7uQ9RnPz9nUaeMZup7K8lyCKyPv2NMTUWvzjqeHpkWC/8pZ6+HWW8rzvatN+7aLnBuoK+GtXoh4aTyfsYz4X5ejaduibgoiIOEoKIiLiKCmIiIijpCAiIo6SgoiIOEoKIiLiKCmIiIjTdp/C9sYAjV975j+Zsf9c/Qhd+z9v/mUa/8in7qPx0dCuXT82d4CufWBiBY0zM3VeE5zw1GEnMqTXYIHXrUeeuvZkmdeut2bs9ekWn+feLPH31UN6DSLPsxpCVtcOIPaMi+/vmDdjvh6JQoHXl3fm7WcDjM3yHofaLO/FSZcbNM6eLdCZ5v0X9ZDf5gPd9vz/fS/207WFYft4A8DCrH2PxJ7rbL7Gj1kqzXt1mnV7/cI833ZlmvfDsF+n/2vne+jSLy29g8a3ep63EJDPlZ6An4926JuCiIg4SgoiIuIoKYiIiKOkICIijpKCiIg4SgoiIuIoKYiIiNN2n0IAT4E4cf26W2j8PxUuovHb/tv7aXzthT8zYxt7NtO1KztGafze3ceZMVYvDAC5DO8VYOICr8FO1Hmfgmd8PxJF+3wGTU+vgOdZD3MLdp112dMLMDVTpPF8kdfzs16ESiND1y7unKHxWdKXsmqAX0e7c/yZIuyZBgB/tsD2efuZHwDQ8vR+ZEkPRKKTH2+fVM6+B1qe5yH4egnSuzz1/AX7/ownPD0QNd5DwT4On9rMe59eHrKfowL4P1fmQ/s6LKY9D0Npg74piIiIo6QgIiKOkoKIiDhKCiIi4igpiIiIo6QgIiJO2yWpzZj/aC2288tki48V/q/v/Qsa//f1/0jjP/nvp5mx1K/x0s7VhT00/mRxsRnbPdpD14ZFXtaWL9nlY9V9/JglKzyfR3X+2nGPXSroO9fJque1ST1sPs3LdGdT/HyVyfhqgI/eDiN+TLoyfAQ1KxUMErxku6fAt11rpGl8Ysq+HsZ28RJH5Pi+pQt22WlilJdu1od4+WR3Z8WMTYb8Gg+rnvHxvMIYMTnd2TF+Ddf7+TFLkpLVvsfoUjx4zkoaP6v0Ao2/GC4yY77P2nbom4KIiDhKCiIi4igpiIiIo6QgIiKOkoKIiDhKCiIi4igpiIiIk4jjmBcai4jIO4a+KYiIiKOkICIijpKCiIg4SgoiIuIoKYiIiKOkICIijpKCiIg4SgoiIuIoKYiIiPN/AAJ9dlrpnLdtAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}